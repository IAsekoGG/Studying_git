{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  conversation_id                             created_at  \\\n",
      "0  1500000.0     1.500000e+18  2022-02-24 14:01:58 GTB Standard Time   \n",
      "1  1500000.0     1.500000e+18  2022-02-24 14:24:32 GTB Standard Time   \n",
      "2  1500000.0     1.500000e+18  2022-02-24 14:42:53 GTB Standard Time   \n",
      "3  1500000.0     1.500000e+18  2022-02-24 14:50:06 GTB Standard Time   \n",
      "4  1500000.0     1.500000e+18  2022-02-24 14:56:54 GTB Standard Time   \n",
      "\n",
      "         dat      time  timezone      user_id        username  \\\n",
      "0  2/24/2022  14:01:58       300  344563802.0        tinkzorg   \n",
      "1  2/24/2022  14:24:32       300  255471924.0      mfa_russia   \n",
      "2  2/24/2022  14:42:53       300  255471924.0      mfa_russia   \n",
      "3  2/24/2022  14:50:06       300  114718372.0  russianembassy   \n",
      "4  2/24/2022  14:56:54       300  255471924.0      mfa_russia   \n",
      "\n",
      "                            name  place  ... geo source user_rt_id user_rt  \\\n",
      "0  Morgenthau Plan Respecter 40K    NaN  ... NaN    NaN        NaN     NaN   \n",
      "1                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "2                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "3            Russian Embassy, UK    NaN  ... NaN    NaN        NaN     NaN   \n",
      "4                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "\n",
      "  retweet_id  reply_to  retweet_date  translate trans_src trans_dest  \n",
      "0        NaN        []           NaN        NaN       NaN        NaN  \n",
      "1        NaN        []           NaN        NaN       NaN        NaN  \n",
      "2        NaN        []           NaN        NaN       NaN        NaN  \n",
      "3        NaN        []           NaN        NaN       NaN        NaN  \n",
      "4        NaN        []           NaN        NaN       NaN        NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "             id  conversation_id                             created_at  \\\n",
      "0  1.500000e+18     1.500000e+18  2022-02-24 02:01:32 GTB Standard Time   \n",
      "1  1.500000e+18     1.500000e+18  2022-02-24 02:02:33 GTB Standard Time   \n",
      "2  1.500000e+18     1.500000e+18  2022-02-24 02:03:39 GTB Standard Time   \n",
      "3  1.500000e+18     1.500000e+18  2022-02-24 02:15:19 GTB Standard Time   \n",
      "4  1.500000e+18     1.500000e+18  2022-02-24 02:19:59 GTB Standard Time   \n",
      "\n",
      "         dat     time  timezone       user_id   username       name  \\\n",
      "0  2/24/2022  2:01:32       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "1  2/24/2022  2:02:33       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "2  2/24/2022  2:03:39       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "3  2/24/2022  2:15:19       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "4  2/24/2022  2:19:59       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "\n",
      "                                               place  ... geo source  \\\n",
      "0  {'type': 'Point', 'coordinates': [40.74881, -7...  ... NaN    NaN   \n",
      "1                                                NaN  ... NaN    NaN   \n",
      "2                                                NaN  ... NaN    NaN   \n",
      "3  {'type': 'Point', 'coordinates': [46.6931, 30....  ... NaN    NaN   \n",
      "4                                                NaN  ... NaN    NaN   \n",
      "\n",
      "  user_rt_id user_rt retweet_id  reply_to  retweet_date  translate trans_src  \\\n",
      "0        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "1        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "2        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "3        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "4        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "\n",
      "  trans_dest  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0494\\AppData\\Local\\Temp\\ipykernel_15004\\3204944764.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_prop['dataset'] = 0  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_prop —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 0\n",
      "C:\\Users\\a0494\\AppData\\Local\\Temp\\ipykernel_15004\\3204944764.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_western['dataset'] = 1  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_western —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = r\"D:\\Andrii\\Diploma\\data\\russian_propaganda_tweets\\russian_propaganda_tweets_v2.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(df.head())\n",
    "dataset_path = r\"D:\\Andrii\\Diploma\\data\\russian_propaganda_tweets\\western_analysts_tweets.csv\"\n",
    "\n",
    "df1 = pd.read_csv(dataset_path, encoding='latin_1')\n",
    "print(df1.head())\n",
    "\n",
    "columns_to_keep = ['reply_to', 'language', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'tweet']\n",
    "df_prop = df[columns_to_keep]\n",
    "#print(df_prop.head())\n",
    "\n",
    "columns_to_keep = ['reply_to', 'language', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'tweet']\n",
    "df_western = df1[columns_to_keep]\n",
    "df_western.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_prop['dataset'] = 0  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_prop —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 0\n",
    "df_western['dataset'] = 1  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_western —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 1\n",
    "\n",
    "df_merged = pd.concat([df_prop, df_western])  # –û–±'—î–¥–Ω—É—î–º–æ –¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∏\n",
    "df_merged = df_merged[df_merged['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Assuming you have a train dataset, you can replace it with your actual dataset\n",
    "columns_to_keep = ['tweet', 'dataset']\n",
    "df_merged_cropped = df_merged[columns_to_keep]\n",
    "train_dataset, test_dataset = train_test_split(df_merged_cropped, test_size=0.2, random_state=42)\n",
    "\n",
    "# Word count filter\n",
    "train_dataset['word_count'] = train_dataset['tweet'].apply(lambda x: len(str(x).split()))\n",
    "train_dataset = train_dataset[train_dataset['word_count'] <= 30]\n",
    "train_dataset = train_dataset.drop(columns=['word_count'])\n",
    "\n",
    "# Word count filter for test dataset\n",
    "test_dataset['word_count'] = test_dataset['tweet'].apply(lambda x: len(str(x).split()))\n",
    "test_dataset = test_dataset[test_dataset['word_count'] <= 30]\n",
    "test_dataset = test_dataset.drop(columns=['word_count'])\n",
    "\n",
    "# TextVectorization\n",
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=30)\n",
    "encoder.adapt(np.array(train_dataset['tweet']))\n",
    "\n",
    "# Save feature names\n",
    "feature_names = encoder.get_vocabulary()\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "X_train_sequences = encoder(np.array(train_dataset['tweet'])).numpy()\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding='post', truncating='post')\n",
    "\n",
    "# –í–∞—à –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∫–æ–¥\n",
    "# –í–∏ –º–æ–∂–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ X_train_padded –≤ —è–∫–æ—Å—Ç—ñ –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∞—à–æ—ó –º–æ–¥–µ–ª—ñ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "454/454 [==============================] - 17s 33ms/step - loss: 0.4426 - accuracy: 0.7986 - val_loss: 0.3698 - val_accuracy: 0.8416\n",
      "Epoch 2/5\n",
      "454/454 [==============================] - 16s 36ms/step - loss: 0.2565 - accuracy: 0.8982 - val_loss: 0.4132 - val_accuracy: 0.8466\n",
      "Epoch 3/5\n",
      "454/454 [==============================] - 15s 32ms/step - loss: 0.1789 - accuracy: 0.9324 - val_loss: 0.3529 - val_accuracy: 0.8526\n",
      "Epoch 4/5\n",
      "454/454 [==============================] - 16s 36ms/step - loss: 0.1348 - accuracy: 0.9485 - val_loss: 0.4749 - val_accuracy: 0.8331\n",
      "Epoch 5/5\n",
      "454/454 [==============================] - 15s 33ms/step - loss: 0.1046 - accuracy: 0.9582 - val_loss: 0.5307 - val_accuracy: 0.8427\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.5195 - accuracy: 0.8358\n",
      "Test accuracy: 0.8357953429222107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "#from keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Model\n",
    "'''\n",
    "model = Sequential([\n",
    "    encoder,\n",
    "    Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=128, mask_zero=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "'''\n",
    "X_train = X_train_padded\n",
    "\n",
    "model = Sequential([Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=128, input_length=30),\n",
    "                    LSTM(128),\n",
    "                    Dense(64, activation='relu'),\n",
    "                    Dense(1, activation='sigmoid')])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "\n",
    "y_train = np.array(train_dataset['dataset'])\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test dataset\n",
    "X_test = np.array(test_dataset['tweet'])\n",
    "y_test = np.array(test_dataset['dataset'])\n",
    "X_test_sequences = encoder(np.array(test_dataset['tweet'])).numpy()\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding='post', truncating='post')\n",
    "\n",
    "# Evaluate on test dataset\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "454/454 [==============================] - 14s 29ms/step - loss: 0.4607 - accuracy: 0.7827 - val_loss: 0.4207 - val_accuracy: 0.8069\n",
      "Epoch 2/5\n",
      "454/454 [==============================] - 13s 28ms/step - loss: 0.3013 - accuracy: 0.8737 - val_loss: 0.3594 - val_accuracy: 0.8452\n",
      "Epoch 3/5\n",
      "454/454 [==============================] - 13s 28ms/step - loss: 0.2215 - accuracy: 0.9105 - val_loss: 0.3884 - val_accuracy: 0.8493\n",
      "Epoch 4/5\n",
      "454/454 [==============================] - 14s 30ms/step - loss: 0.1817 - accuracy: 0.9262 - val_loss: 0.4162 - val_accuracy: 0.8457\n",
      "Epoch 5/5\n",
      "454/454 [==============================] - 20s 43ms/step - loss: 0.1545 - accuracy: 0.9365 - val_loss: 0.4973 - val_accuracy: 0.8460\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.8401\n",
      "Test accuracy: 0.8400770425796509\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "#from keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Model\n",
    "model = Sequential([\n",
    "    encoder,\n",
    "    Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=256, mask_zero=True),\n",
    "    Dropout(0.5),  #–¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó\n",
    "    BatchNormalization(),  # –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –≤ –∫–æ–∂–Ω–æ–º—É –º—ñ–Ω—ñ-–±–∞—Ç—á—ñ\n",
    "    GlobalAveragePooling1D(), #–æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ –≤—Å—ñ—Ö –≤–∏—Ö—ñ–¥–Ω–∏—Ö —Ñ—ñ—á–∞—Ö\n",
    "    Dense(128, activation='relu'),  #Dense-—à–∞—Ä\n",
    "    Dropout(0.5),  # –î–æ–¥–∞–Ω–∏–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X_train = np.array(train_dataset['tweet'])\n",
    "y_train = np.array(train_dataset['dataset'])\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test dataset\n",
    "X_test = np.array(test_dataset['tweet'])\n",
    "y_test = np.array(test_dataset['dataset'])\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequences\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# –ó–∞–º—ñ–Ω—ñ—Ç—å MAX_SEQ_LEN –∑–Ω–∞—á–µ–Ω–Ω—è–º, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ–π –¥–æ–≤–∂–∏–Ω—ñ —Ç–µ–∫—Å—Ç—É\u001B[39;00m\n\u001B[0;32m      4\u001B[0m MAX_SEQ_LEN \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30\u001B[39m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}