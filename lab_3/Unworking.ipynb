{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb648a3-c5ee-4d43-b24e-309aec404756",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 17:00:01.304030: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-28 17:00:01.386925: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-28 17:00:01.795022: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/hanrewan/anaconda3/lib/:/home/hanrewan/anaconda3/lib/python3.8/site-packages/tensorrt/:/home/hanrewan/anaconda3/envs/tf/lib:/home/hanrewan/anaconda3/envs/tf/lib/python3.8/site-packages/tensorrt/:/home/hanrewan/miniconda3/envs/tf/lib/\n",
      "2023-10-28 17:00:01.795060: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/hanrewan/anaconda3/lib/:/home/hanrewan/anaconda3/lib/python3.8/site-packages/tensorrt/:/home/hanrewan/anaconda3/envs/tf/lib:/home/hanrewan/anaconda3/envs/tf/lib/python3.8/site-packages/tensorrt/:/home/hanrewan/miniconda3/envs/tf/lib/\n",
      "2023-10-28 17:00:01.795063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69393e4-39a9-4c65-ba64-785a9ea48e16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now patchifying image: segm_data/Tile 6/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 6/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 1/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 8/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 5/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 4/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 7/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 3/images/image_part_001.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_009.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_007.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_003.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_002.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_004.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_006.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_005.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_008.jpg\n",
      "Now patchifying image: segm_data/Tile 2/images/image_part_001.jpg\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 6/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 1/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 8/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 5/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 4/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 7/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 3/masks/image_part_003.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_004.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_006.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_005.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_008.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_007.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_009.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_001.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_002.png\n",
      "Now patchifying mask: segm_data/Tile 2/masks/image_part_003.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import cv2\n",
    "from patchify import patchify\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "patch_size = 256\n",
    "root_directory = 'segm_data'\n",
    "\n",
    "image_dataset = []  \n",
    "for path, subdirs, files in os.walk(root_directory):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "    if dirname == 'images':   #Find all 'images' directories\n",
    "        images = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, image_name in enumerate(images):  \n",
    "            if image_name.endswith(\".jpg\"):   #Only read jpg images...\n",
    "               \n",
    "                image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
    "                SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                image = Image.fromarray(image)\n",
    "                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                image = np.array(image)             \n",
    "       \n",
    "                #Extract patches from each image\n",
    "                print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
    "                patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_img.shape[0]):\n",
    "                    for j in range(patches_img.shape[1]):\n",
    "                        \n",
    "                        single_patch_img = patches_img[i,j,:,:]\n",
    "                        \n",
    "                        #Use minmaxscaler instead of just dividing by 255. \n",
    "                        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n",
    "                        \n",
    "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. \n",
    "                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "                        image_dataset.append(single_patch_img)\n",
    "                \n",
    "  \n",
    "                \n",
    "  \n",
    " #Now do the same as above for masks\n",
    " #For this specific dataset we could have added masks to the above code as masks have extension png\n",
    "mask_dataset = []  \n",
    "for path, subdirs, files in os.walk(root_directory):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "    if dirname == 'masks':   #Find all 'images' directories\n",
    "        masks = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, mask_name in enumerate(masks):  \n",
    "            if mask_name.endswith(\".png\"):   #Only read png images... (masks in this dataset)\n",
    "               \n",
    "                mask = cv2.imread(path+\"/\"+mask_name, 1)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "                mask = cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n",
    "                SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "                mask = Image.fromarray(mask)\n",
    "                mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                mask = np.array(mask)             \n",
    "       \n",
    "                #Extract patches from each image\n",
    "                print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
    "                patches_mask = patchify(mask, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_mask.shape[0]):\n",
    "                    for j in range(patches_mask.shape[1]):\n",
    "                        \n",
    "                        single_patch_mask = patches_mask[i,j,:,:]\n",
    "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "                        single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "                        mask_dataset.append(single_patch_mask) \n",
    "\n",
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset =  np.array(mask_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206e388f-ca42-422a-834d-6db279a80ac8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]],\n",
       "\n",
       "       [[110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]],\n",
       "\n",
       "       [[110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        [110, 193, 228],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]],\n",
       "\n",
       "       [[132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]],\n",
       "\n",
       "       [[132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        [132,  41, 246],\n",
       "        ...,\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152],\n",
       "        [ 60,  16, 152]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5e16d-202c-4e6c-8784-bdcee92e2c02",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7464388c-c44f-4cef-8069-6512c645fbea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, concatenate, BatchNormalization, UpSampling2D, Cropping2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "  \n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = next_layer  \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3, \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d429c52-8aa4-4b4c-bf4f-32399af4ec28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "def multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.2)(c1)  # Original 0.1\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.2)(c2)  # Original 0.1\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.2)(c8)  # Original 0.1\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.2)(c9)  # Original 0.1\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af58541-bd50-4539-b23d-1e58ed2d48a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#unet = multi_unet_model(IMG_CHANNELS=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01dea0d-67af-4fd9-8276-10b4ee873373",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 17:00:08.727211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.730128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.730212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.730801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-28 17:00:08.732323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.732418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.732496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.938400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.938548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.938613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-28 17:00:08.938673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2100 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(256, 256, 3))\n",
    "encoder_output1, skip1 = EncoderMiniBlock(input_layer, n_filters=32)\n",
    "encoder_output2, skip2 = EncoderMiniBlock(encoder_output1, n_filters=64)\n",
    "decoder_output2 = DecoderMiniBlock(encoder_output2, skip1, n_filters=64)\n",
    "decoder_output1 = DecoderMiniBlock(decoder_output2, input_layer, n_filters=32)\n",
    "\n",
    "#unet = Model(inputs=input_layer, outputs=decoder_output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379ab866-5b34-495e-9c7b-544d84cac622",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_dataset, mask_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97bac595-0e8a-47b9-b3f2-3d4f5b8fdc4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 17:00:09.356674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb73133-76d5-4675-b490-0faa74351628",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c878ff-4660-4378-954c-b8c7831dc8db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rgb_to_2D_label(label):\n",
    "    \"\"\"\n",
    "    Suply our labale masks as input in RGB format. \n",
    "    Replace pixels with specific RGB values ...\n",
    "    \"\"\"\n",
    "    label_seg = np.zeros(label.shape,dtype=np.uint8)\n",
    "    label_seg [np.all(label == Building,axis=-1)] = 0\n",
    "    label_seg [np.all(label==Land,axis=-1)] = 1\n",
    "    label_seg [np.all(label==Road,axis=-1)] = 2\n",
    "    label_seg [np.all(label==Vegetation,axis=-1)] = 3\n",
    "    label_seg [np.all(label==Water,axis=-1)] = 4\n",
    "    label_seg [np.all(label==Unlabeled,axis=-1)] = 5\n",
    "    \n",
    "    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels\n",
    "    \n",
    "    return label_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2911fa45-bfa9-40d8-9b7e-b466ee72670c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Building = '#3C1098'.lstrip('#')\n",
    "Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n",
    "\n",
    "Land = '#8429F6'.lstrip('#')\n",
    "Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n",
    "\n",
    "Road = '#6EC1E4'.lstrip('#') \n",
    "Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n",
    "\n",
    "Vegetation =  'FEDD3A'.lstrip('#') \n",
    "Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n",
    "\n",
    "Water = 'E2A929'.lstrip('#') \n",
    "Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n",
    "\n",
    "Unlabeled = '#9B9B9B'.lstrip('#') \n",
    "Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n",
    "\n",
    "label = single_patch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f4deb2-406e-46f7-b3de-f7673bc5552c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(mask_dataset.shape[0]):\n",
    "    label = rgb_to_2D_label(mask_dataset[i])\n",
    "    labels.append(label)    \n",
    "\n",
    "labels = np.array(labels)   \n",
    "labels = np.expand_dims(labels, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "271a72b8-c6be-49fa-9db5-f696ed46a25c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a301c4-b728-4cce-80b9-54183f2bc24d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 17:28:10.449761: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.12MiB (rounded to 1179648)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-10-28 17:28:10.449789: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-10-28 17:28:10.449796: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 92, Chunks in use: 92. 23.0KiB allocated for chunks. 23.0KiB in use in bin. 8.8KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449798: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 15, Chunks in use: 14. 8.0KiB allocated for chunks. 7.2KiB in use in bin. 6.8KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449801: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 8, Chunks in use: 8. 10.5KiB allocated for chunks. 10.5KiB in use in bin. 10.1KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449803: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 3.5KiB allocated for chunks. 3.5KiB in use in bin. 3.4KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449805: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449808: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 9, Chunks in use: 7. 88.2KiB allocated for chunks. 69.2KiB in use in bin. 61.0KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449810: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 7, Chunks in use: 5. 130.0KiB allocated for chunks. 94.0KiB in use in bin. 90.0KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449813: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 9, Chunks in use: 9. 320.0KiB allocated for chunks. 320.0KiB in use in bin. 316.0KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449815: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 10, Chunks in use: 8. 772.0KiB allocated for chunks. 576.0KiB in use in bin. 543.4KiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449817: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 13, Chunks in use: 11. 1.95MiB allocated for chunks. 1.60MiB in use in bin. 1.59MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449820: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 7, Chunks in use: 5. 2.03MiB allocated for chunks. 1.41MiB in use in bin. 1.41MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449822: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 9, Chunks in use: 7. 5.00MiB allocated for chunks. 3.88MiB in use in bin. 3.81MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449824: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 5, Chunks in use: 5. 5.62MiB allocated for chunks. 5.62MiB in use in bin. 5.62MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449827: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449829: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449831: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449833: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449837: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449839: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449841: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 195.75MiB allocated for chunks. 195.75MiB in use in bin. 195.75MiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449844: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 3. 1.84GiB allocated for chunks. 1.84GiB in use in bin. 1.72GiB client-requested in use in bin.\n",
      "2023-10-28 17:28:10.449846: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 1.12MiB was 1.00MiB, Chunk State: \n",
      "2023-10-28 17:28:10.449849: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 2202468352\n",
      "2023-10-28 17:28:10.449853: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000000 of size 1280 next 1\n",
      "2023-10-28 17:28:10.449855: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000500 of size 256 next 2\n",
      "2023-10-28 17:28:10.449857: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000600 of size 256 next 3\n",
      "2023-10-28 17:28:10.449858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000700 of size 256 next 5\n",
      "2023-10-28 17:28:10.449860: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000800 of size 256 next 6\n",
      "2023-10-28 17:28:10.449861: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000900 of size 256 next 4\n",
      "2023-10-28 17:28:10.449863: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000a00 of size 256 next 11\n",
      "2023-10-28 17:28:10.449864: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000b00 of size 256 next 9\n",
      "2023-10-28 17:28:10.449866: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000c00 of size 256 next 10\n",
      "2023-10-28 17:28:10.449868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000d00 of size 256 next 14\n",
      "2023-10-28 17:28:10.449869: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000e00 of size 256 next 15\n",
      "2023-10-28 17:28:10.449871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50000f00 of size 256 next 16\n",
      "2023-10-28 17:28:10.449872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001000 of size 256 next 17\n",
      "2023-10-28 17:28:10.449874: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001100 of size 256 next 18\n",
      "2023-10-28 17:28:10.449875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001200 of size 256 next 23\n",
      "2023-10-28 17:28:10.449877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001300 of size 256 next 21\n",
      "2023-10-28 17:28:10.449878: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001400 of size 256 next 22\n",
      "2023-10-28 17:28:10.449880: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001500 of size 256 next 26\n",
      "2023-10-28 17:28:10.449881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001600 of size 256 next 27\n",
      "2023-10-28 17:28:10.449883: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001700 of size 256 next 28\n",
      "2023-10-28 17:28:10.449884: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001800 of size 256 next 29\n",
      "2023-10-28 17:28:10.449886: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001900 of size 256 next 32\n",
      "2023-10-28 17:28:10.449887: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001a00 of size 256 next 30\n",
      "2023-10-28 17:28:10.449889: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001b00 of size 256 next 31\n",
      "2023-10-28 17:28:10.449891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001c00 of size 256 next 34\n",
      "2023-10-28 17:28:10.449892: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001d00 of size 256 next 37\n",
      "2023-10-28 17:28:10.449894: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001e00 of size 256 next 39\n",
      "2023-10-28 17:28:10.449895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50001f00 of size 256 next 40\n",
      "2023-10-28 17:28:10.449897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002000 of size 256 next 41\n",
      "2023-10-28 17:28:10.449898: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002100 of size 256 next 43\n",
      "2023-10-28 17:28:10.449900: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002200 of size 256 next 46\n",
      "2023-10-28 17:28:10.449901: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002300 of size 256 next 45\n",
      "2023-10-28 17:28:10.449903: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002400 of size 256 next 7\n",
      "2023-10-28 17:28:10.449904: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50002500 of size 3584 next 8\n",
      "2023-10-28 17:28:10.449906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003300 of size 256 next 71\n",
      "2023-10-28 17:28:10.449908: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003400 of size 1024 next 67\n",
      "2023-10-28 17:28:10.449910: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003800 of size 256 next 70\n",
      "2023-10-28 17:28:10.449911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003900 of size 256 next 76\n",
      "2023-10-28 17:28:10.449913: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003a00 of size 512 next 77\n",
      "2023-10-28 17:28:10.449915: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003c00 of size 512 next 78\n",
      "2023-10-28 17:28:10.449916: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50003e00 of size 512 next 80\n",
      "2023-10-28 17:28:10.449918: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004000 of size 256 next 82\n",
      "2023-10-28 17:28:10.449919: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004100 of size 256 next 84\n",
      "2023-10-28 17:28:10.449921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004200 of size 256 next 85\n",
      "2023-10-28 17:28:10.449922: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004300 of size 256 next 86\n",
      "2023-10-28 17:28:10.449924: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004400 of size 256 next 88\n",
      "2023-10-28 17:28:10.449925: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004500 of size 256 next 90\n",
      "2023-10-28 17:28:10.449927: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004600 of size 256 next 92\n",
      "2023-10-28 17:28:10.449928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004700 of size 256 next 93\n",
      "2023-10-28 17:28:10.449930: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004800 of size 256 next 94\n",
      "2023-10-28 17:28:10.449931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004900 of size 256 next 96\n",
      "2023-10-28 17:28:10.449933: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004a00 of size 256 next 98\n",
      "2023-10-28 17:28:10.449935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004b00 of size 256 next 100\n",
      "2023-10-28 17:28:10.449936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50004c00 of size 12032 next 53\n",
      "2023-10-28 17:28:10.449938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50007b00 of size 18432 next 54\n",
      "2023-10-28 17:28:10.449940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000c300 of size 256 next 175\n",
      "2023-10-28 17:28:10.449941: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000c400 of size 256 next 180\n",
      "2023-10-28 17:28:10.449943: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000c500 of size 256 next 177\n",
      "2023-10-28 17:28:10.449944: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000c600 of size 256 next 179\n",
      "2023-10-28 17:28:10.449946: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000c700 of size 768 next 173\n",
      "2023-10-28 17:28:10.449947: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000ca00 of size 1792 next 174\n",
      "2023-10-28 17:28:10.449949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd5000d100 of size 5632 next 124\n",
      "2023-10-28 17:28:10.449951: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5000e700 of size 9216 next 125\n",
      "2023-10-28 17:28:10.449952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50010b00 of size 8192 next 163\n",
      "2023-10-28 17:28:10.449954: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd50012b00 of size 10240 next 12\n",
      "2023-10-28 17:28:10.449955: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50015300 of size 36864 next 13\n",
      "2023-10-28 17:28:10.449957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5001e300 of size 36864 next 47\n",
      "2023-10-28 17:28:10.449959: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027300 of size 256 next 51\n",
      "2023-10-28 17:28:10.449960: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027400 of size 256 next 44\n",
      "2023-10-28 17:28:10.449962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027500 of size 256 next 50\n",
      "2023-10-28 17:28:10.449963: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027600 of size 256 next 59\n",
      "2023-10-28 17:28:10.449965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027700 of size 256 next 55\n",
      "2023-10-28 17:28:10.449966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027800 of size 512 next 64\n",
      "2023-10-28 17:28:10.449968: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027a00 of size 256 next 58\n",
      "2023-10-28 17:28:10.449970: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027b00 of size 512 next 63\n",
      "2023-10-28 17:28:10.449972: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50027d00 of size 1024 next 48\n",
      "2023-10-28 17:28:10.449973: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028100 of size 1792 next 49\n",
      "2023-10-28 17:28:10.449975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028800 of size 256 next 102\n",
      "2023-10-28 17:28:10.449977: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028900 of size 256 next 105\n",
      "2023-10-28 17:28:10.449978: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028a00 of size 256 next 101\n",
      "2023-10-28 17:28:10.449980: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028b00 of size 256 next 103\n",
      "2023-10-28 17:28:10.449981: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028c00 of size 256 next 108\n",
      "2023-10-28 17:28:10.449983: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028d00 of size 256 next 110\n",
      "2023-10-28 17:28:10.449984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028e00 of size 256 next 111\n",
      "2023-10-28 17:28:10.449986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50028f00 of size 256 next 109\n",
      "2023-10-28 17:28:10.449987: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029000 of size 256 next 112\n",
      "2023-10-28 17:28:10.449989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029100 of size 256 next 115\n",
      "2023-10-28 17:28:10.449990: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029200 of size 256 next 116\n",
      "2023-10-28 17:28:10.449992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029300 of size 256 next 113\n",
      "2023-10-28 17:28:10.449994: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029400 of size 512 next 114\n",
      "2023-10-28 17:28:10.449995: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029600 of size 256 next 119\n",
      "2023-10-28 17:28:10.449997: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029700 of size 256 next 121\n",
      "2023-10-28 17:28:10.449998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029800 of size 256 next 122\n",
      "2023-10-28 17:28:10.450000: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029900 of size 256 next 120\n",
      "2023-10-28 17:28:10.450001: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029a00 of size 256 next 129\n",
      "2023-10-28 17:28:10.450003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029b00 of size 256 next 126\n",
      "2023-10-28 17:28:10.450004: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029c00 of size 256 next 133\n",
      "2023-10-28 17:28:10.450006: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029d00 of size 512 next 128\n",
      "2023-10-28 17:28:10.450007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50029f00 of size 512 next 137\n",
      "2023-10-28 17:28:10.450009: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5002a100 of size 1024 next 132\n",
      "2023-10-28 17:28:10.450010: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5002a500 of size 1024 next 104\n",
      "2023-10-28 17:28:10.450012: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5002a900 of size 9984 next 52\n",
      "2023-10-28 17:28:10.450013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5002d000 of size 13056 next 19\n",
      "2023-10-28 17:28:10.450015: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50030300 of size 73728 next 20\n",
      "2023-10-28 17:28:10.450017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50042300 of size 73728 next 42\n",
      "2023-10-28 17:28:10.450018: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50054300 of size 73728 next 38\n",
      "2023-10-28 17:28:10.450020: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50066300 of size 147456 next 24\n",
      "2023-10-28 17:28:10.450022: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5008a300 of size 147456 next 25\n",
      "2023-10-28 17:28:10.450023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500ae300 of size 147456 next 33\n",
      "2023-10-28 17:28:10.450025: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d2300 of size 18432 next 106\n",
      "2023-10-28 17:28:10.450026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d6b00 of size 1792 next 123\n",
      "2023-10-28 17:28:10.450028: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7200 of size 512 next 141\n",
      "2023-10-28 17:28:10.450029: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7400 of size 512 next 142\n",
      "2023-10-28 17:28:10.450031: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7600 of size 512 next 136\n",
      "2023-10-28 17:28:10.450033: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7800 of size 256 next 151\n",
      "2023-10-28 17:28:10.450035: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7900 of size 256 next 148\n",
      "2023-10-28 17:28:10.450036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7a00 of size 256 next 150\n",
      "2023-10-28 17:28:10.450038: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7b00 of size 256 next 157\n",
      "2023-10-28 17:28:10.450039: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7c00 of size 256 next 154\n",
      "2023-10-28 17:28:10.450041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7d00 of size 256 next 156\n",
      "2023-10-28 17:28:10.450042: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7e00 of size 256 next 160\n",
      "2023-10-28 17:28:10.450044: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d7f00 of size 256 next 162\n",
      "2023-10-28 17:28:10.450045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8000 of size 256 next 164\n",
      "2023-10-28 17:28:10.450047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8100 of size 512 next 183\n",
      "2023-10-28 17:28:10.450048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8300 of size 256 next 186\n",
      "2023-10-28 17:28:10.450050: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8400 of size 256 next 191\n",
      "2023-10-28 17:28:10.450051: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd500d8500 of size 768 next 170\n",
      "2023-10-28 17:28:10.450053: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8800 of size 512 next 171\n",
      "2023-10-28 17:28:10.450054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8a00 of size 256 next 167\n",
      "2023-10-28 17:28:10.450056: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8b00 of size 256 next 168\n",
      "2023-10-28 17:28:10.450057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8c00 of size 256 next 166\n",
      "2023-10-28 17:28:10.450059: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8d00 of size 256 next 172\n",
      "2023-10-28 17:28:10.450060: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8e00 of size 256 next 107\n",
      "2023-10-28 17:28:10.450062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500d8f00 of size 9216 next 56\n",
      "2023-10-28 17:28:10.450063: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500db300 of size 36864 next 57\n",
      "2023-10-28 17:28:10.450065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500e4300 of size 32768 next 95\n",
      "2023-10-28 17:28:10.450067: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd500ec300 of size 9216 next 176\n",
      "2023-10-28 17:28:10.450068: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500ee700 of size 9216 next 165\n",
      "2023-10-28 17:28:10.450070: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500f0b00 of size 18432 next 161\n",
      "2023-10-28 17:28:10.450071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500f5300 of size 36864 next 130\n",
      "2023-10-28 17:28:10.450073: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd500fe300 of size 40960 next 35\n",
      "2023-10-28 17:28:10.450075: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50108300 of size 221184 next 36\n",
      "2023-10-28 17:28:10.450076: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5013e300 of size 73728 next 60\n",
      "2023-10-28 17:28:10.450078: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50150300 of size 36864 next 99\n",
      "2023-10-28 17:28:10.450079: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd50159300 of size 18432 next 127\n",
      "2023-10-28 17:28:10.450081: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd5015db00 of size 18432 next 97\n",
      "2023-10-28 17:28:10.450083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50162300 of size 73728 next 61\n",
      "2023-10-28 17:28:10.450085: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50174300 of size 147456 next 62\n",
      "2023-10-28 17:28:10.450086: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50198300 of size 131072 next 87\n",
      "2023-10-28 17:28:10.450088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd501b8300 of size 73728 next 159\n",
      "2023-10-28 17:28:10.450089: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd501ca300 of size 73728 next 155\n",
      "2023-10-28 17:28:10.450091: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd501dc300 of size 147456 next 134\n",
      "2023-10-28 17:28:10.450093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50200300 of size 163840 next 65\n",
      "2023-10-28 17:28:10.450094: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50228300 of size 294912 next 66\n",
      "2023-10-28 17:28:10.450096: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50270300 of size 147456 next 91\n",
      "2023-10-28 17:28:10.450097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50294300 of size 32768 next 158\n",
      "2023-10-28 17:28:10.450099: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd5029c300 of size 18432 next 178\n",
      "2023-10-28 17:28:10.450101: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd502a0b00 of size 22528 next 131\n",
      "2023-10-28 17:28:10.450102: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd502a6300 of size 73728 next 89\n",
      "2023-10-28 17:28:10.450104: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd502b8300 of size 294912 next 68\n",
      "2023-10-28 17:28:10.450105: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50300300 of size 589824 next 69\n",
      "2023-10-28 17:28:10.450107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50390300 of size 524288 next 79\n",
      "2023-10-28 17:28:10.450109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50410300 of size 73728 next 182\n",
      "2023-10-28 17:28:10.450110: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd50422300 of size 221184 next 153\n",
      "2023-10-28 17:28:10.450112: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50458300 of size 294912 next 149\n",
      "2023-10-28 17:28:10.450114: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd504a0300 of size 589824 next 138\n",
      "2023-10-28 17:28:10.450115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50530300 of size 655360 next 72\n",
      "2023-10-28 17:28:10.450117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd505d0300 of size 1179648 next 73\n",
      "2023-10-28 17:28:10.450118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd506f0300 of size 589824 next 83\n",
      "2023-10-28 17:28:10.450120: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50780300 of size 131072 next 152\n",
      "2023-10-28 17:28:10.450122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd507a0300 of size 36864 next 181\n",
      "2023-10-28 17:28:10.450123: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd507a9300 of size 126976 next 135\n",
      "2023-10-28 17:28:10.450125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd507c8300 of size 294912 next 81\n",
      "2023-10-28 17:28:10.450126: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50810300 of size 1179648 next 74\n",
      "2023-10-28 17:28:10.450128: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50930300 of size 2359296 next 75\n",
      "2023-10-28 17:28:10.450130: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd50b70300 of size 821035008 next 117\n",
      "2023-10-28 17:28:10.450131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd81a70300 of size 205258752 next 118\n",
      "2023-10-28 17:28:10.450133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8de30300 of size 524288 next 145\n",
      "2023-10-28 17:28:10.450135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd8deb0300 of size 147456 next 184\n",
      "2023-10-28 17:28:10.450136: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8ded4300 of size 147456 next 185\n",
      "2023-10-28 17:28:10.450138: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd8def8300 of size 360448 next 139\n",
      "2023-10-28 17:28:10.450140: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8df50300 of size 1179648 next 140\n",
      "2023-10-28 17:28:10.450141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd8e070300 of size 294912 next 187\n",
      "2023-10-28 17:28:10.450143: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e0b8300 of size 294912 next 188\n",
      "2023-10-28 17:28:10.450144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd8e100300 of size 589824 next 146\n",
      "2023-10-28 17:28:10.450146: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e190300 of size 1179648 next 147\n",
      "2023-10-28 17:28:10.450148: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7efd8e2b0300 of size 589824 next 189\n",
      "2023-10-28 17:28:10.450149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e340300 of size 589824 next 190\n",
      "2023-10-28 17:28:10.450151: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e3d0300 of size 1179648 next 143\n",
      "2023-10-28 17:28:10.450152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e4f0300 of size 2359296 next 144\n",
      "2023-10-28 17:28:10.450154: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efd8e730300 of size 821035008 next 169\n",
      "2023-10-28 17:28:10.450156: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7efdbf630300 of size 333708544 next 18446744073709551615\n",
      "2023-10-28 17:28:10.450157: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-10-28 17:28:10.450177: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 92 Chunks of size 256 totalling 23.0KiB\n",
      "2023-10-28 17:28:10.450179: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 13 Chunks of size 512 totalling 6.5KiB\n",
      "2023-10-28 17:28:10.450181: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 768 totalling 768B\n",
      "2023-10-28 17:28:10.450183: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 1024 totalling 4.0KiB\n",
      "2023-10-28 17:28:10.450184: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-10-28 17:28:10.450186: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 1792 totalling 5.2KiB\n",
      "2023-10-28 17:28:10.450188: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2023-10-28 17:28:10.450190: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8192 totalling 8.0KiB\n",
      "2023-10-28 17:28:10.450191: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 9216 totalling 27.0KiB\n",
      "2023-10-28 17:28:10.450193: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 9984 totalling 9.8KiB\n",
      "2023-10-28 17:28:10.450195: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12032 totalling 11.8KiB\n",
      "2023-10-28 17:28:10.450197: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13056 totalling 12.8KiB\n",
      "2023-10-28 17:28:10.450198: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 18432 totalling 72.0KiB\n",
      "2023-10-28 17:28:10.450200: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 22528 totalling 22.0KiB\n",
      "2023-10-28 17:28:10.450202: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 32768 totalling 64.0KiB\n",
      "2023-10-28 17:28:10.450204: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 36864 totalling 216.0KiB\n",
      "2023-10-28 17:28:10.450205: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 40960 totalling 40.0KiB\n",
      "2023-10-28 17:28:10.450207: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 73728 totalling 576.0KiB\n",
      "2023-10-28 17:28:10.450209: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 131072 totalling 256.0KiB\n",
      "2023-10-28 17:28:10.450211: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 147456 totalling 1008.0KiB\n",
      "2023-10-28 17:28:10.450213: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 163840 totalling 160.0KiB\n",
      "2023-10-28 17:28:10.450215: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 221184 totalling 216.0KiB\n",
      "2023-10-28 17:28:10.450217: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 294912 totalling 1.41MiB\n",
      "2023-10-28 17:28:10.450218: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 524288 totalling 1.00MiB\n",
      "2023-10-28 17:28:10.450220: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 589824 totalling 2.25MiB\n",
      "2023-10-28 17:28:10.450222: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 655360 totalling 640.0KiB\n",
      "2023-10-28 17:28:10.450224: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 1179648 totalling 5.62MiB\n",
      "2023-10-28 17:28:10.450226: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 2359296 totalling 4.50MiB\n",
      "2023-10-28 17:28:10.450228: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 205258752 totalling 195.75MiB\n",
      "2023-10-28 17:28:10.450230: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 333708544 totalling 318.25MiB\n",
      "2023-10-28 17:28:10.450231: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 821035008 totalling 1.53GiB\n",
      "2023-10-28 17:28:10.450233: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 2.05GiB\n",
      "2023-10-28 17:28:10.450235: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 2202468352 memory_limit_: 2202468352 available bytes: 0 curr_region_allocation_bytes_: 4404936704\n",
      "2023-10-28 17:28:10.450239: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                      2202468352\n",
      "InUse:                      2200001280\n",
      "MaxInUse:                   2200001280\n",
      "NumAllocs:                         446\n",
      "MaxAllocSize:                821035008\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-10-28 17:28:10.450245: W tensorflow/tsl/framework/bfc_allocator.cc:492] ***********************************************************************************************xxxxx\n",
      "2023-10-28 17:28:10.450863: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_model\u001B[39m():\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m multi_unet_model(n_classes\u001B[38;5;241m=\u001B[39mn_classes, IMG_HEIGHT\u001B[38;5;241m=\u001B[39mIMG_HEIGHT, IMG_WIDTH\u001B[38;5;241m=\u001B[39mIMG_WIDTH, IMG_CHANNELS\u001B[38;5;241m=\u001B[39mIMG_CHANNELS)\n\u001B[0;32m---> 22\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39mtotal_loss, metrics\u001B[38;5;241m=\u001B[39mmetrics)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[18], line 20\u001B[0m, in \u001B[0;36mget_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_model\u001B[39m():\n\u001B[0;32m---> 20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmulti_unet_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIMG_HEIGHT\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIMG_HEIGHT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIMG_WIDTH\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIMG_WIDTH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIMG_CHANNELS\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIMG_CHANNELS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 41\u001B[0m, in \u001B[0;36mmulti_unet_model\u001B[0;34m(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\u001B[0m\n\u001B[1;32m     38\u001B[0m c4 \u001B[38;5;241m=\u001B[39m Conv2D(\u001B[38;5;241m128\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, kernel_initializer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhe_normal\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m)(c4)\n\u001B[1;32m     39\u001B[0m p4 \u001B[38;5;241m=\u001B[39m MaxPooling2D(pool_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m))(c4)\n\u001B[0;32m---> 41\u001B[0m c5 \u001B[38;5;241m=\u001B[39m \u001B[43mConv2D\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrelu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_initializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhe_normal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msame\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp4\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m c5 \u001B[38;5;241m=\u001B[39m Dropout(\u001B[38;5;241m0.3\u001B[39m)(c5)\n\u001B[1;32m     43\u001B[0m c5 \u001B[38;5;241m=\u001B[39m Conv2D(\u001B[38;5;241m256\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, kernel_initializer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhe_normal\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m)(c5)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py:2142\u001B[0m, in \u001B[0;36mRandomGenerator.truncated_normal\u001B[0;34m(self, shape, mean, stddev, dtype, nonce)\u001B[0m\n\u001B[1;32m   2140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nonce:\n\u001B[1;32m   2141\u001B[0m         seed \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mstateless_fold_in(seed, nonce)\n\u001B[0;32m-> 2142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstateless_truncated_normal\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstddev\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstddev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\n\u001B[1;32m   2144\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mtruncated_normal(\n\u001B[1;32m   2146\u001B[0m     shape\u001B[38;5;241m=\u001B[39mshape,\n\u001B[1;32m   2147\u001B[0m     mean\u001B[38;5;241m=\u001B[39mmean,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2150\u001B[0m     seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_legacy_seed(),\n\u001B[1;32m   2151\u001B[0m )\n",
      "\u001B[0;31mResourceExhaustedError\u001B[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=weights) \n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss) \n",
    "\n",
    "\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    " \n",
    "\n",
    "metrics=['accuracy', jacard_coef]\n",
    "\n",
    "def get_model():\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss=total_loss, metrics=metrics)\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history1 = model.fit(X_train, y_train, \n",
    "                    batch_size = 16, \n",
    "                    verbose=1, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06aa83a-5aee-4235-a73c-f47d1498c4cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}