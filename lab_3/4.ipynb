{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  conversation_id                             created_at  \\\n",
      "0  1500000.0     1.500000e+18  2022-02-24 14:01:58 GTB Standard Time   \n",
      "1  1500000.0     1.500000e+18  2022-02-24 14:24:32 GTB Standard Time   \n",
      "2  1500000.0     1.500000e+18  2022-02-24 14:42:53 GTB Standard Time   \n",
      "3  1500000.0     1.500000e+18  2022-02-24 14:50:06 GTB Standard Time   \n",
      "4  1500000.0     1.500000e+18  2022-02-24 14:56:54 GTB Standard Time   \n",
      "\n",
      "         dat      time  timezone      user_id        username  \\\n",
      "0  2/24/2022  14:01:58       300  344563802.0        tinkzorg   \n",
      "1  2/24/2022  14:24:32       300  255471924.0      mfa_russia   \n",
      "2  2/24/2022  14:42:53       300  255471924.0      mfa_russia   \n",
      "3  2/24/2022  14:50:06       300  114718372.0  russianembassy   \n",
      "4  2/24/2022  14:56:54       300  255471924.0      mfa_russia   \n",
      "\n",
      "                            name  place  ... geo source user_rt_id user_rt  \\\n",
      "0  Morgenthau Plan Respecter 40K    NaN  ... NaN    NaN        NaN     NaN   \n",
      "1                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "2                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "3            Russian Embassy, UK    NaN  ... NaN    NaN        NaN     NaN   \n",
      "4                  MFA Russia üá∑üá∫    NaN  ... NaN    NaN        NaN     NaN   \n",
      "\n",
      "  retweet_id  reply_to  retweet_date  translate trans_src trans_dest  \n",
      "0        NaN        []           NaN        NaN       NaN        NaN  \n",
      "1        NaN        []           NaN        NaN       NaN        NaN  \n",
      "2        NaN        []           NaN        NaN       NaN        NaN  \n",
      "3        NaN        []           NaN        NaN       NaN        NaN  \n",
      "4        NaN        []           NaN        NaN       NaN        NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "             id  conversation_id                             created_at  \\\n",
      "0  1.500000e+18     1.500000e+18  2022-02-24 02:01:32 GTB Standard Time   \n",
      "1  1.500000e+18     1.500000e+18  2022-02-24 02:02:33 GTB Standard Time   \n",
      "2  1.500000e+18     1.500000e+18  2022-02-24 02:03:39 GTB Standard Time   \n",
      "3  1.500000e+18     1.500000e+18  2022-02-24 02:15:19 GTB Standard Time   \n",
      "4  1.500000e+18     1.500000e+18  2022-02-24 02:19:59 GTB Standard Time   \n",
      "\n",
      "         dat     time  timezone       user_id   username       name  \\\n",
      "0  2/24/2022  2:01:32       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "1  2/24/2022  2:02:33       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "2  2/24/2022  2:03:39       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "3  2/24/2022  2:15:19       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "4  2/24/2022  2:19:59       300  2.388075e+09  liveuamap  Liveuamap   \n",
      "\n",
      "                                               place  ... geo source  \\\n",
      "0  {'type': 'Point', 'coordinates': [40.74881, -7...  ... NaN    NaN   \n",
      "1                                                NaN  ... NaN    NaN   \n",
      "2                                                NaN  ... NaN    NaN   \n",
      "3  {'type': 'Point', 'coordinates': [46.6931, 30....  ... NaN    NaN   \n",
      "4                                                NaN  ... NaN    NaN   \n",
      "\n",
      "  user_rt_id user_rt retweet_id  reply_to  retweet_date  translate trans_src  \\\n",
      "0        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "1        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "2        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "3        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "4        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
      "\n",
      "  trans_dest  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0494\\AppData\\Local\\Temp\\ipykernel_21364\\161055038.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_prop['dataset'] = 0  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_prop —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 0\n",
      "C:\\Users\\a0494\\AppData\\Local\\Temp\\ipykernel_21364\\161055038.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_western['dataset'] = 1  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_western —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = r\"C:\\Users\\a0494\\Desktop\\Diploma\\data\\russian_propaganda_tweets\\russian_propaganda_tweets_v2.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(df.head())\n",
    "dataset_path = r\"C:\\Users\\a0494\\Desktop\\Diploma\\data\\russian_propaganda_tweets\\western_analysts_tweets.csv\"\n",
    "\n",
    "df1 = pd.read_csv(dataset_path, encoding='latin_1')\n",
    "print(df1.head())\n",
    "\n",
    "columns_to_keep = ['reply_to', 'language', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'tweet']\n",
    "df_prop = df[columns_to_keep]\n",
    "#print(df_prop.head())\n",
    "\n",
    "columns_to_keep = ['reply_to', 'language', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'tweet']\n",
    "df_western = df1[columns_to_keep]\n",
    "df_western.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_prop['dataset'] = 0  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_prop —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 0\n",
    "df_western['dataset'] = 1  # –î–æ–¥–∞—î–º–æ —Å—Ç–æ–≤–ø–µ—Ü—å 'dataset' –¥–æ df_western —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ 1\n",
    "\n",
    "df_merged = pd.concat([df_prop, df_western])  # –û–±'—î–¥–Ω—É—î–º–æ –¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∏\n",
    "df_merged = df_merged[df_merged['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assuming you have a train dataset, you can replace it with your actual dataset\n",
    "columns_to_keep = ['tweet', 'dataset']\n",
    "df_merged_cropped = df_merged[columns_to_keep]\n",
    "train_dataset, test_dataset = train_test_split(df_merged_cropped, test_size=0.2, random_state=42)\n",
    "\n",
    "# Word count filter\n",
    "train_dataset['word_count'] = train_dataset['tweet'].apply(lambda x: len(str(x).split()))\n",
    "train_dataset = train_dataset[train_dataset['word_count'] <= 30]\n",
    "train_dataset = train_dataset.drop(columns=['word_count'])\n",
    "\n",
    "# Word count filter for test dataset\n",
    "test_dataset['word_count'] = test_dataset['tweet'].apply(lambda x: len(str(x).split()))\n",
    "test_dataset = test_dataset[test_dataset['word_count'] <= 30]\n",
    "test_dataset = test_dataset.drop(columns=['word_count'])\n",
    "# TextVectorization\n",
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.array(train_dataset['tweet']))\n",
    "\n",
    "# Save feature names\n",
    "feature_names = encoder.get_vocabulary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "454/454 [==============================] - 10s 18ms/step - loss: 0.5173 - accuracy: 0.7618 - val_loss: 0.4329 - val_accuracy: 0.8039\n",
      "Epoch 2/5\n",
      "454/454 [==============================] - 8s 18ms/step - loss: 0.3308 - accuracy: 0.8640 - val_loss: 0.3703 - val_accuracy: 0.8394\n",
      "Epoch 3/5\n",
      "454/454 [==============================] - 8s 18ms/step - loss: 0.2425 - accuracy: 0.9059 - val_loss: 0.3529 - val_accuracy: 0.8534\n",
      "Epoch 4/5\n",
      "454/454 [==============================] - 8s 17ms/step - loss: 0.1889 - accuracy: 0.9295 - val_loss: 0.3548 - val_accuracy: 0.8567\n",
      "Epoch 5/5\n",
      "454/454 [==============================] - 9s 19ms/step - loss: 0.1528 - accuracy: 0.9454 - val_loss: 0.3693 - val_accuracy: 0.8573\n",
      "146/146 [==============================] - 1s 4ms/step - loss: 0.3832 - accuracy: 0.8495\n",
      "Test accuracy: 0.8494969010353088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "#from keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Model\n",
    "model = Sequential([\n",
    "    encoder,\n",
    "    Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=128, mask_zero=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X_train = np.array(train_dataset['tweet'])\n",
    "y_train = np.array(train_dataset['dataset'])\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test dataset\n",
    "X_test = np.array(test_dataset['tweet'])\n",
    "y_test = np.array(test_dataset['dataset'])\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "454/454 [==============================] - 14s 29ms/step - loss: 0.4607 - accuracy: 0.7827 - val_loss: 0.4207 - val_accuracy: 0.8069\n",
      "Epoch 2/5\n",
      "454/454 [==============================] - 13s 28ms/step - loss: 0.3013 - accuracy: 0.8737 - val_loss: 0.3594 - val_accuracy: 0.8452\n",
      "Epoch 3/5\n",
      "454/454 [==============================] - 13s 28ms/step - loss: 0.2215 - accuracy: 0.9105 - val_loss: 0.3884 - val_accuracy: 0.8493\n",
      "Epoch 4/5\n",
      "454/454 [==============================] - 14s 30ms/step - loss: 0.1817 - accuracy: 0.9262 - val_loss: 0.4162 - val_accuracy: 0.8457\n",
      "Epoch 5/5\n",
      "454/454 [==============================] - 20s 43ms/step - loss: 0.1545 - accuracy: 0.9365 - val_loss: 0.4973 - val_accuracy: 0.8460\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.8401\n",
      "Test accuracy: 0.8400770425796509\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "#from keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Model\n",
    "model = Sequential([\n",
    "    encoder,\n",
    "    Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=256, mask_zero=True),\n",
    "    Dropout(0.5),  # –î–æ–¥–∞–Ω–∏–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó\n",
    "    BatchNormalization(),  # –î–æ–¥–∞–Ω–∏–π BatchNormalization –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ —Ç–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu'),  # –î–æ–¥–∞–Ω–∏–π Dense-—à–∞—Ä –∑ —Ñ—É–Ω–∫—Ü—ñ—î—é –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó ReLU\n",
    "    Dropout(0.5),  # –î–æ–¥–∞–Ω–∏–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X_train = np.array(train_dataset['tweet'])\n",
    "y_train = np.array(train_dataset['dataset'])\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test dataset\n",
    "X_test = np.array(test_dataset['tweet'])\n",
    "y_test = np.array(test_dataset['dataset'])\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}